# -*- coding: utf-8 -*-
"""Segmentation01_Resnet-18

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BRfLnczkEi4H0NifNh0rJrcA8cGGc3rS
"""

import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision.models import resnet18
from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

class LungSegmentationModel(nn.Module):
    def __init__(self, num_classes=2):
        super(LungSegmentationModel, self).__init__()

        # Load ResNet-18 as the backbone
        self.backbone = resnet18(pretrained=True)

        # Replace the last layer to output num_classes channels
        num_filters = self.backbone.fc.in_features
        self.backbone.fc = nn.Conv2d(num_filters, num_classes, kernel_size=1)

    def forward(self, x):
        # Pass input through the backbone
        x = self.backbone.conv1(x)
        x = self.backbone.bn1(x)
        x = self.backbone.relu(x)
        x = self.backbone.maxpool(x)

        x = self.backbone.layer1(x)
        x = self.backbone.layer2(x)
        x = self.backbone.layer3(x)
        x = self.backbone.layer4(x)

        # Upsample the output to the original input size
        x = F.interpolate(x, scale_factor=4, mode='bilinear', align_corners=False)

        # Output segmentation mask
        x = self.backbone.fc(x)

        # Upsample the output to the same size as the label tensor
        x = F.interpolate(x, size=labels.shape[1:], mode='bilinear', align_corners=False)

        return x

model = LungSegmentationModel(num_classes=2)

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

from PIL import Image
from torch.utils.data import Dataset, DataLoader
from torchvision.transforms import Compose, Resize, ToTensor

class LungDataset(Dataset):
    def __init__(self, img_folder, label_folder, transform=None):
        self.img_folder = img_folder
        self.label_folder = label_folder
        self.transform = transform
        self.img_names = os.listdir(img_folder)

    def __len__(self):
        return len(self.img_names)

    def __getitem__(self, idx):
        img_path = os.path.join(self.img_folder, self.img_names[idx])
        label_path = os.path.join(self.label_folder, self.img_names[idx])
        
        img = Image.open(img_path).convert('RGB')
        label = Image.open(label_path).convert('L')

        if self.transform:
            img = self.transform(img)
            label = self.transform(label)
       
       # Remove the channel dimension from the label tensor
        label = label[0]

        return img, label



transform = Compose([
    Resize((256, 256)),
    ToTensor()
])

train_dataset = LungDataset('/content/drive/My Drive/Liang/Segmentation01/train/org', '/content/drive/My Drive/Liang/Segmentation01/train/label', transform=transform)
test_dataset = LungDataset('/content/drive/My Drive/Liang/Segmentation01/test/org', '/content/drive/My Drive/Liang/Segmentation01/test/label', transform=transform)

train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=4, shuffle=True)

# Move model to GPU
if torch.cuda.is_available():
    model.cuda()


# Training loop
num_epochs = 10

train_loss = 0.0

for epoch in range(num_epochs):
    for i, (images, labels) in enumerate(train_loader):
        # Move images and labels to GPU
        images = images.cuda()
        labels = labels.cuda()

        # Zero out gradients
        optimizer.zero_grad()

        # Forward pass
        outputs = model(images)

        # Compute loss
        loss = criterion(outputs, labels.long())

        # Backward pass and optimization step
        loss.backward()
        optimizer.step()

        train_loss += loss.item()

        # Print loss every 10 batches
        if i % 10 == 0:
            print(f"Epoch {epoch+1}/{num_epochs}, Batch {i+1}/{len(train_loader)}, Loss: {loss.item()}")

from sklearn.metrics import confusion_matrix
import numpy as np

# IoU function
def iou_score(outputs, labels):
    intersection = np.logical_and(outputs, labels)
    union = np.logical_or(outputs, labels)
    iou = np.sum(intersection) / np.sum(union)
    return iou

# Dice Coefficient function
def dice_coefficient(outputs, labels):
    intersection = np.logical_and(outputs, labels)
    dice = 2 * np.sum(intersection) / (np.sum(outputs) + np.sum(labels))
    return dice

# Pixel Accuracy function
def pixel_accuracy(outputs, labels):
    pixel_acc = np.sum(outputs == labels) / np.prod(outputs.shape)
    return pixel_acc

# Evaluate the model on the test set
model.eval()
with torch.no_grad():
    iou_scores = []
    dice_scores = []
    pixel_accs = []

    for images, labels in test_loader:
        # Move images and labels to GPU
        images = images.cuda()
        labels = labels.cuda()

        # Forward pass
        outputs = model(images)

        # Convert output to binary mask
        outputs = torch.argmax(outputs, dim=1)
        outputs = outputs.cpu().numpy()

        # Calculate IoU, Dice Coefficient, and Pixel Accuracy
        labels = labels.cpu().numpy()
        iou = iou_score(outputs, labels)
        dice = dice_coefficient(outputs, labels)
        pixel_acc = pixel_accuracy(outputs, labels)

        # Append scores to lists
        iou_scores.append(iou)
        dice_scores.append(dice)
        pixel_accs.append(pixel_acc)

    # Print average scores
    print('Average IoU: {:.4f}'.format(np.mean(iou_scores)))
    print('Average Dice Coefficient: {:.4f}'.format(np.mean(dice_scores)))
    print('Average Pixel Accuracy: {:.4f}'.format(np.mean(pixel_accs)))


    # Evaluate on test set after each epoch
    model.eval()
    with torch.no_grad():
        test_loss = 0
        for images, labels in test_loader:
            # Move images and labels to GPU
            images = images.cuda()
            labels = labels.cuda()

            # Forward pass
            outputs = model(images)

            # Compute loss
            test_loss += criterion(outputs, labels.long()).item()

        test_loss /= len(test_loader)
        print(f"Epoch {epoch+1}/{num_epochs}, Test Loss: {test_loss}")
    model.train()