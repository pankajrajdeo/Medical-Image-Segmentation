# -*- coding: utf-8 -*-
"""Segmentation02_Resnet-18

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1p35NJU3R38O_IlhXOWGJvrCtOZ-OthMD
"""

import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision.models import resnet18
from google.colab import drive
import numpy as np
from torchvision.transforms import Compose, Resize, ToTensor, ToPILImage
import matplotlib.pyplot as plt
from torchvision.transforms import ToPILImage

def display_predictions(images, labels, outputs, img_ids, iou, iou_threshold):
    correct_img_ids = []
    for idx in range(images.shape[0]):
        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))
        
        # Original image
        ax1.imshow(images[idx].permute(1, 2, 0).cpu().numpy())
        ax1.set_title(f"Original Image (ID: {img_ids[idx]})")
        
        # Ground truth
        ax2.imshow(labels[idx].cpu().numpy(), cmap="gray")
        ax2.set_title("Ground Truth")
        
        # Predicted mask
        ax3.imshow(outputs[idx], cmap="gray")
        
        # Print correct or incorrect prediction
        if iou[idx] > iou_threshold:
            ax3.set_title("Correct Prediction")
            print(f"Correct Prediction for Image ID: {img_ids[idx]}")
            correct_img_ids.append(img_ids[idx])
        else:
            ax3.set_title("Incorrect Prediction")
            print(f"Incorrect Prediction for Image ID: {img_ids[idx]}")
        
        plt.show()
    
    return correct_img_ids



# Mount Google Drive
drive.mount('/content/drive')

class LungSegmentationModel(nn.Module):
    def __init__(self, num_classes=4):
        super(LungSegmentationModel, self).__init__()

        # Load ResNet-18 as the backbone
        self.backbone = resnet18(pretrained=True)

        # Replace the last layer to output num_classes channels
        num_filters = self.backbone.fc.in_features
        self.backbone.fc = nn.Conv2d(num_filters, num_classes, kernel_size=1)

    def forward(self, x):
        _, _, H, W = x.shape

        # Pass input through the backbone
        x = self.backbone.conv1(x)
        x = self.backbone.bn1(x)
        x = self.backbone.relu(x)
        x = self.backbone.maxpool(x)

        x = self.backbone.layer1(x)
        x = self.backbone.layer2(x)
        x = self.backbone.layer3(x)
        x = self.backbone.layer4(x)

        # Upsample the output to the original input size
        x = F.interpolate(x, scale_factor=4, mode='bilinear', align_corners=False)

        # Output segmentation mask
        x = self.backbone.fc(x)

        # Upsample the output to the same size as the label tensor
        x = F.interpolate(x, size=(H, W), mode='bilinear', align_corners=False)

        return x


model = LungSegmentationModel(num_classes=4)

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)


from PIL import Image
from torch.utils.data import Dataset, DataLoader
from torchvision.transforms import Compose, Resize, ToTensor

class LungDataset(Dataset):
    def __init__(self, img_folder, label_folder, transform=None):
        self.img_folder = img_folder
        self.label_folder = label_folder
        self.transform = transform
        self.img_names = os.listdir(img_folder)

    def __len__(self):
        return len(self.img_names)

    def __getitem__(self, idx):
        img_path = os.path.join(self.img_folder, self.img_names[idx])
        label_path = os.path.join(self.label_folder, self.img_names[idx].replace('.bmp', '_label.png'))

        img = Image.open(img_path).convert('RGB')
        label = Image.open(label_path).convert('L')

        # Convert label to a tensor of 0's and 1's
        label = torch.tensor(np.array(label) > 0, dtype=torch.long)

        if self.transform:
            img = self.transform(img)
       
        return img, label

transform = Compose([
    Resize((256, 256)),
    ToTensor()
])

train_dataset = LungDataset('/content/drive/My Drive/Liang/segmentation02/segmentation/org_train', '/content/drive/My Drive/Liang/segmentation02/segmentation/label_train', transform=transform)
test_dataset = LungDataset('/content/drive/My Drive/Liang/segmentation02/segmentation/org_test', '/content/drive/My Drive/Liang/segmentation02/segmentation/label_test', transform=transform)

train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)

# Move model to GPU
if torch.cuda.is_available():
    model.cuda()


# Training loop
num_epochs = 10

train_loss = 0.0

for epoch in range(num_epochs):
    for i, (images, labels) in enumerate(train_loader):
        # Move images and labels to GPU
        images = images.cuda()
        labels = labels.cuda()

        # Zero out gradients
        optimizer.zero_grad()

        # Forward pass
        outputs = model(images)

        # Compute loss
        loss = criterion(outputs, labels.long())

        # Backward pass and optimization step
        loss.backward()
        optimizer.step()

        train_loss += loss.item()

        # Print loss every 10 batches
        if i % 10 == 0:
            print(f"Epoch {epoch+1}/{num_epochs}, Batch {i+1}/{len(train_loader)}, Loss: {loss.item()}")

from sklearn.metrics import confusion_matrix
import numpy as np

# IoU function
def iou_score(outputs, labels):
    intersection = np.logical_and(outputs, labels)
    union = np.logical_or(outputs, labels)
    iou = np.sum(intersection) / np.sum(union)
    return iou

# Dice Coefficient function
def dice_coefficient(outputs, labels):
    intersection = np.logical_and(outputs, labels)
    dice = 2 * np.sum(intersection) / (np.sum(outputs) + np.sum(labels))
    return dice

# Pixel Accuracy function
def pixel_accuracy(outputs, labels):
    pixel_acc = np.sum(outputs == labels) / np.prod(outputs.shape)
    return pixel_acc

# Evaluate the model on the test set
model.eval()
with torch.no_grad():
    iou_scores = []
    dice_scores = []
    pixel_accs = []

    for images, labels in test_loader:
        # Move images and labels to GPU
        images = images.cuda()
        labels = labels.cuda()

        # Forward pass
        outputs = model(images)

        # Convert output to binary mask
        outputs = torch.argmax(outputs, dim=1)
        outputs = outputs.cpu().numpy()

        # Calculate IoU, Dice Coefficient, and Pixel Accuracy
        labels = labels.cpu().numpy()
        iou = iou_score(outputs, labels)
        dice = dice_coefficient(outputs, labels)
        pixel_acc = pixel_accuracy(outputs, labels)

        # Append scores to lists
        iou_scores.append(iou)
        dice_scores.append(dice)
        pixel_accs.append(pixel_acc)

    # Print average scores
    print('Average IoU: {:.4f}'.format(np.mean(iou_scores)))
    print('Average Dice Coefficient: {:.4f}'.format(np.mean(dice_scores)))
    print('Average Pixel Accuracy: {:.4f}'.format(np.mean(pixel_accs)))


# Evaluate on test set after each epoch
model.eval()
iou_threshold = 0.98  # Set the IoU threshold
net_ids = []  # Initialize the net_ids list
with torch.no_grad():
    test_loss = 0
    correct = 0
    total = 0
    iou_scores = []

    for images, labels in test_loader:
        # Move images and labels to GPU
        images = images.cuda()
        labels = labels.cuda()

        # Forward pass
        outputs = model(images)

        # Convert output to binary mask
        outputs = torch.argmax(outputs, dim=1)
        outputs = outputs.cpu().numpy()

        # Calculate IoU, Dice Coefficient, and Pixel Accuracy
        labels = labels.cpu().numpy()
        iou = [iou_score(outputs[i], labels[i]) for i in range(outputs.shape[0])]
        iou_scores.extend(iou)

        img_ids = [test_dataset.img_names[i].split('.')[0] for i in range(outputs.shape[0])]
        
        # Display predictions using the modified display_predictions function
        correct_img_ids = display_predictions(images, torch.from_numpy(labels), outputs, img_ids, iou, iou_threshold)

        # Update net_ids list with the correct_img_ids
        net_ids.extend(correct_img_ids)

        # Update total and correct
        total += len(outputs)
        correct += sum(i > iou_threshold for i in iou)

    # Print average scores
    print('Average IoU: {:.4f}'.format(np.mean(iou_scores)))
    print('Average Dice Coefficient: {:.4f}'.format(np.mean(dice_scores)))
    print('Average Pixel Accuracy: {:.4f}'.format(np.mean(pixel_accs)))
    print('Test Accuracy: {:.4f}'.format(correct/total))

    # Print net ids of correct predictions
    print('Net IDs of Correct Predictions:', net_ids)
    
model.train()